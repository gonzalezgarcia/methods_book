[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Research methods in Cognitive Neuroscience",
    "section": "",
    "text": "Welcome\n\n\n\nAI algorithm hallucinates what this course is about\n\n\nThis text is aimed to serve as your guide for the first half of the course “Methodology in Cognitive Neuroscience: Basic and Applied Research” in the Master’s Program in Cognitive and Behavioral Neuroscience at the University of Granada.\nIn this section, we will explore the foundations of experimental research in cognitive neuroscience and develop practical skills in experiment programming. We’ll begin by introducing key concepts in research methods and experimental thinking, providing you with a solid theoretical foundation. Then, we’ll dive into hands-on training using E-Prime and OpenSesame, two powerful pieces of software for creating psychology experiments.\nBy the end of this module, you will be able to:\n\nUnderstand and apply principles of experimental design in cognitive neuroscience\nThink critically about research methodology and experimental control\nDesign and program your own experiments using E-Prime and OpenSesame\nTroubleshoot common issues in experiment programming\n\nThis guide is designed to be both theoretical and practical. Most of the chapters include conceptual discussions followed by hands-on exercises to help you apply what you’ve learned. Remember, mastering these skills requires practice, creativity, and persistence!\n\n\n\n\n\n\nRelevant links\n\n\n\nCourse guide\n\n\n\n\n\n\n\n\nDisclaimer\n\n\n\nThis material has been elaborated for the use of the Cognitive Neuroscience master students of the Universidad de Granada, years 2023-25.\nThe resources listed here are Open Educational Resources (OER) that are free to use, share, copy, and edit, with attribution, following the terms of the specified license.\nPlease contact Carlos González for any inquiry."
  },
  {
    "objectID": "syllabus.html#schedule",
    "href": "syllabus.html#schedule",
    "title": "Syllabus",
    "section": "Schedule",
    "text": "Schedule\nClasses will take place at Seminario 3 (CIMCYC) on Wednesday from 17:00 to 18:50 and Fridays from 09:00 to 10:50."
  },
  {
    "objectID": "syllabus.html#required-software",
    "href": "syllabus.html#required-software",
    "title": "Syllabus",
    "section": "Required Software",
    "text": "Required Software\nCheck PRADO for instructions on how to install E-Prime.\nOpenSesame (download latest version from here).\nIt should run in any more or less recent computer. If you have any issue installing it, please let me know as soon as possible!"
  },
  {
    "objectID": "syllabus.html#resources-and-recommended-readings",
    "href": "syllabus.html#resources-and-recommended-readings",
    "title": "Syllabus",
    "section": "Resources and recommended readings",
    "text": "Resources and recommended readings\n\n\n\n\n\n\nNote\n\n\n\nNote: these are just extra readings in case you want to learn more. They are encouraged but not required to follow or pass the course.\n\n\n\nBarbosa, J., Stein, H., Zorowitz, S., Niv, Y., Summerfield, C., Soto-Faraco, S., & Hyafil, A. (2023). A practical guide for studying human behavior in the lab. Behavior Research Methods, 55(1), 58-76.\nFrank, M. C., Braginsky, M., Cachia, J., Coles, N.A., Hardwicke, T.E., Hawkins, R.E., Mathur, M.B., and Williams, R. 2024. Experimentology: An Open Science Approach to Experimental Psychology Methods. MIT Press. https://doi.org/10.7551/mitpress/14810.001.0001.\nMathôt, S., Schreij, D., & Theeuwes, J. (2012). OpenSesame: An open-source, graphical experiment builder for the social sciences. Behavior Research Methods, 44, 314-324.\nMyers, J. L., Well, A. D., & Lorch Jr, R. F. (2013). Research design and statistical analysis. Routledge."
  },
  {
    "objectID": "syllabus.html#assessment",
    "href": "syllabus.html#assessment",
    "title": "Syllabus",
    "section": "Assessment",
    "text": "Assessment\nThis course is divided in two parts:\n\nProgramming of experiments (50% of the final grade)\nStatistical analyses (50% of the final grade)\n\n\n\n\n\n\n\nImportant!\n\n\n\nA minimum of 25% in each phase is required to pass the course.\n\n\nIn my part of the course (Part 1), your final grade will depend on:\n\n\n\nActivity\nContribution to final grade\n\n\n\n\nParticipation and in-class assignments\n30%\n\n\nIndividual programming assignments\n30%\n\n\nFinal project\n40%"
  },
  {
    "objectID": "syllabus.html#course-policies",
    "href": "syllabus.html#course-policies",
    "title": "Syllabus",
    "section": "Course Policies",
    "text": "Course Policies\n\nAttendance and Participation\nAttendance is strongly encouraged for this course due to its eminently practical nature. Please note:\n\nMany in-class activities and hands-on exercises might not be easily replicated outside of class.\nRegular attendance will significantly enhance your learning experience and ability to complete assignments successfully.\nIf you must miss a class, it is your responsibility to catch up on missed material and assignments.\nConsistent participation in class discussions and activities will positively impact your learning and final grade.\n\n\n\nLate Work and Extensions\n\nAssignments are due on the dates specified in the course schedule.\nLate submissions will incur a 20% penalty\nIf you anticipate difficulty meeting a deadline, please contact me as soon as possible to discuss potential extensions.\nExtensions may be granted for documented emergencies or extenuating circumstances at the instructor’s discretion.\n\n\n\nAcademic Integrity\n\nAll work submitted must be your own.\nWhen using external sources (including generative AI tools), proper citation is required.\nCollaboration on assignments is encouraged, but each student must submit their own original work.\n\n\n\nCommunication\n\nEmail (cgonzalez at ugr dot es) is the primary mode of communication outside of class.\nYou can also use PRADO if you prefer."
  },
  {
    "objectID": "syllabus.html#office-hours",
    "href": "syllabus.html#office-hours",
    "title": "Syllabus",
    "section": "Office Hours",
    "text": "Office Hours\nMondays from 8:30 to 11, but feel free to send me an email anytime or just ask me after class."
  },
  {
    "objectID": "s0.html#from-research-questions-to-data",
    "href": "s0.html#from-research-questions-to-data",
    "title": "Session 0: Course kick-off",
    "section": "From Research Questions to Data",
    "text": "From Research Questions to Data\nThe essence of cognitive neuroscience experiments is to examine relationships between manipulations (independent variables) and their effects on measurable outcomes (dependent variables). Remember there are various experimental designs, such as:\n\nBetween-participants design: Each participant experiences only one level of a factor.\nWithin-participants design: Participants experience multiple levels of a factor, allowing for within-subject comparisons.\nMixed designs: Combining elements of both between- and within-participant designs.\n\nThere are also techniques like counterbalancing to mitigate order effects and ensure reliable, replicable results."
  },
  {
    "objectID": "s0.html#tools-for-experimental-design",
    "href": "s0.html#tools-for-experimental-design",
    "title": "Session 0: Course kick-off",
    "section": "Tools for Experimental Design",
    "text": "Tools for Experimental Design\nDuring the programming phase, you’ll become familiar with two key software tools:\n\nE-Prime\n\nWidely used at CIMCYC\nGreat for users with little coding experience\nOnly available on Windows (License required; check PRADO)\n\n\n\nOpenSesame\n\nPython-based and open source\nCross-platform and free\nRequires basic coding knowledge, but has a large online community\n\nIf you need help accessing these tools or have a non-Windows computer, feel free to reach out!\n\n\n\n\n\n\nRelevant readings\n\n\n\nFor more reading, check out the following resources:\n\nMyers, Well, & Lorch (2013). Research Design and Statistical Analysis\nExperimentology\nBarbosa (2022)\n\n\n\n\n\n\n\n\n\nFood for thought\n\n\n\n\n\n\n1. Understanding Experimental Design\n\nWhat are the key differences between between-participants and within-participants designs, and in what scenarios would you prefer one over the other?\nReflect on the importance of counterbalancing in experimental design. How does it help improve the reliability of an experiment?\n\n\n\n2. From Research Questions to Experiment Design\n\nHow would you approach transforming a cognitive neuroscience research question into a concrete experimental procedure? What are the key stages you must consider?\nHow do repeated measures (within-participant designs) affect the interpretation of experimental data compared to between-participant designs?\n\n\n\n3. Programming and Tools\n\nCompare and contrast the strengths and weaknesses of E-Prime and OpenSesame. How might the choice of tool affect the design and execution of your experiment?\nIn programming an experiment, why is it important to understand both the user interface (e.g., E-Prime’s E-Studio) and coding components (e.g., Inline Scripts)?\n\n\n\n4. Ethical Considerations\n\nWhen designing experiments in cognitive neuroscience, what ethical considerations should be taken into account? How can these be incorporated into the design phase?\n\n\n\n\n\nBelow, you can find the slides from this session:"
  },
  {
    "objectID": "s1.html#key-stages-in-experimental-development",
    "href": "s1.html#key-stages-in-experimental-development",
    "title": "Session 1",
    "section": "Key Stages in Experimental Development",
    "text": "Key Stages in Experimental Development\n\nConceptualize the Core Experimental (trial) Procedure\nThis is where you define the fundamental, hierarchical structure of the experiment.\nElaborate the Trial Procedure\nDetermine how each trial will flow and what elements will be presented.\nAdd All Conditions and Set the Number of Trials and Sampling Strategy\nDefine the independent variables, number of trials, and how trials are sampled.\nAdd Blocks and Block Conditions\nBreak the trials into blocks and set conditions for each block.\nAdd a Practice Block\nInclude a practice phase where participants can familiarize themselves with the task.\nTesting and Running the Program\nEnsure the experiment runs smoothly and make any necessary adjustments.\nPerform Basic Data Analysis\nTake a first look at the results once the data is collected to make sure everything looks fine."
  },
  {
    "objectID": "s1.html#developing-a-lexical-decision-task",
    "href": "s1.html#developing-a-lexical-decision-task",
    "title": "Session 1",
    "section": "Developing a Lexical Decision Task",
    "text": "Developing a Lexical Decision Task\nIn this session, we’ll create a lexical decision task, where participants must determine whether a string of letters is a word or a non-word. The task will consist of several trials, each with the following sequence:\n\nFixation: Displayed for 1000ms\n\nProbe: Displayed for 2000ms\n\nFor each trial, the word will either be a real word (e.g., “cat”) or a non-word (e.g., “jop”). The task will require participants to press a specific key based on whether they think the displayed string is a word or non-word.\n\nFixed vs. Varying Properties\nTo understand how E-Prime works, a key concept to always keep in mind is the distinction between fixed and varying properties in an experiment: - Fixed properties: Elements like the fixation display (e.g., position, color, duration) will remain the same across trials. - Varying properties: The actual words and correct responses will change with each trial.\nIf we define which properties remain constant and which vary across trials when designing each event, it will be easir to understand which information goes into a procedure and what goes into a list."
  },
  {
    "objectID": "s1.html#adding-complexity-to-the-experiment",
    "href": "s1.html#adding-complexity-to-the-experiment",
    "title": "Session 1",
    "section": "Adding Complexity to the Experiment",
    "text": "Adding Complexity to the Experiment\nWe will also explore the following adjustments:\n\nBlocks: The experiment will consist of two blocks, with a break in between.\nCounterbalancing: To avoid response bias, responses can be counterbalanced across participants. For example:\n\nParticipant 1: word = “k”, non-word = “l”\nParticipant 2: word = “l”, non-word = “k”"
  },
  {
    "objectID": "s1.html#adding-a-practice-block",
    "href": "s1.html#adding-a-practice-block",
    "title": "Session 1",
    "section": "Adding a Practice Block",
    "text": "Adding a Practice Block\nBefore the actual experiment, we’ll include a practice block with the following key differences: - Feedback will be provided after each trial to help participants learn the task. - Practice trials will be marked separately to exclude them from the final data analysis.\n\n\n\n\n\n\n\nFood for thought\n\n\n\n\nWhy is it essential to define both fixed and varying properties for an experiment? Provide examples of each in the context of a Stroop task.\n\n\n\n\nBelow, you can find the slides from this session:"
  },
  {
    "objectID": "s2.html#developing-a-lexical-decision-task",
    "href": "s2.html#developing-a-lexical-decision-task",
    "title": "Session 1",
    "section": "Developing a Lexical Decision Task",
    "text": "Developing a Lexical Decision Task\n\nTrial Structure\nEach trial in the lexical decision task involves the following sequence:\n\nFixation: Displayed for 1000 ms\nProbe: Displayed for 2000 ms\n\nThe key goal is to understand what elements need to be fixed (e.g., fixation cross, display duration) and what should vary (e.g., the word presented, correct response).\n\n\nAdding Blocks\nTo add more complexity, we can divide the trials into blocks, with breaks in between.\n\n\n\n\n\n\nHomework\n\n\n\nHow would you add a practice block to the task?:::\n\n\n\n\nAdding Feedback to Trials\n\nAdd a FeedbackDisplay object as the last event in your trial procedure.\nExplore the feedback’s default options (e.g., states, duration).\nUse the “Input Object Name” to base the feedback on a specific object (e.g., correct response).\n\n\n\n\n\n\n\nHomework\n\n\n\nHow would you add feedback only to the practice block?\n\n\n\n\nAdding All Trials\nThere are different ways to add trials: - If the number of trials is small, you can manually enter the conditions. - For a large number of trials, use nested lists. This involves: - Reducing the trial list to the number of conditions. - Creating a new list for each condition, which contains the trial exemplars.\n\n\n\n\n\n\nTip\n\n\n\nKeep at least two rows per condition to avoid predictable sequences.\n\n\n\n\nCounterbalancing Responses\nTo prevent response bias, you can counterbalance responses across participants. For example: - Participant 1: word = “k”; non-word = “l” - Participant 2: word = “l”; non-word = “k”"
  },
  {
    "objectID": "s2.html#first-task-submission-the-flanker-task",
    "href": "s2.html#first-task-submission-the-flanker-task",
    "title": "Session 1",
    "section": "First Task Submission: The Flanker Task",
    "text": "First Task Submission: The Flanker Task\nDeadline: October 20th\nIn this task, you will explore the effect of congruence on response inhibition using the Flanker Task, which assesses the ability to suppress inappropriate responses.\n\nKey Elements of the Flanker Task\n\nCongruent Trials: Where flankers and the target match (e.g., HHHHH).\nIncongruent Trials: Where the flankers and the target do not match (e.g., SSHSS).\n\n\nSequence of Events per Trial\n\nFixation (150 ms)\nProbe (80 ms)\nResponse Window (800 ms)\nFeedback (300 ms)\n\n\n\n\nTask Design\nThe experiment will follow a 2 (between-participants) x 2 (within-participants) design: - Between-participants factor: Proportion of congruence (50% vs 80% incongruent trials). - Within-participants factor: Congruence (congruent vs incongruent trials).\nEach participant will complete 80 trials in total, equally distributed across the conditions.\n\n\nProbe List\nYou must use nested lists for random probe selection, ensuring that all probes are used the same number of times across trials.\n\n\n\nCongruent\nIncongruent\n\n\n\n\nAAAAA\nAABAA\n\n\nBBBBB\nBBABB\n\n\nXXXXX\nXXYXX\n\n\nYYYYY\nYYXYY\n\n\n\n\n\nExtra Points\n\nCounterbalance the proportion of congruence between participants (e.g., group 1 = 50%, group 2 = 80%).\nRun three “participants”, merge their data, and compute the mean reaction time (RT) and accuracy (ACC) for each condition.\n\nPlease upload your completed task to PRADO or send it via email. Late submissions will incur a 20% penalty.\n\n\n\n\n\n\n\nFood for thought\n\n\n\n\n\nConsider the following questionsn:\n\nTrial Structure and Experimental Design\n\nWhat are the advantages of using nested lists when designing experiments with a large number of trials or conditions? How does this affect the flexibility and scalability of your experimental design?\nWhy is it important to balance the number of trials across conditions, and what issues could arise if this balance is not maintained?\n\n\n\nFeedback in Cognitive Tasks\n\nHow does adding feedback in a cognitive task, particularly in a practice block, help participants perform better in the actual trials? What are the potential downsides of providing feedback in all blocks?\nReflect on the role of feedback in learning and task performance. Should feedback always be included in experiments, or are there scenarios where it might interfere with data collection?\n\n\n\nCounterbalancing and Bias\n\nWhy is counterbalancing critical in cognitive tasks, such as the lexical decision task or the Flanker task? What would be the consequences of failing to counterbalance responses across participants?\nHow does counterbalancing responses improve the internal validity of an experiment? Can counterbalancing introduce any unintended complexities in data analysis?\n\n\n\nThe Flanker Task\n\nIn the context of the Flanker task, how does the proportion of congruent versus incongruent trials affect response inhibition? Why might a higher proportion of incongruent trials make it easier for participants to control inappropriate responses?\nReflect on how varying the proportion of congruent and incongruent trials could affect the interpretation of reaction time (RT) and accuracy (ACC) results. How might these effects differ across participants with varying cognitive control abilities?\n\n\n\nExperimental Controls and Variability\n\nWhat are some potential challenges in designing experiments with both within-participants and between-participants factors, as seen in the Flanker task? How can you minimize participant variability while maintaining the experimental design’s integrity?\nWhy is it important to control for the sequence and timing of stimuli presentation, and how do randomization and counterbalancing help address these issues?\n\n\n\n\n\n\nBelow, you can find the slides from this session:"
  }
]